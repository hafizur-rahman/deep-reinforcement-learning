{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.5)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "        self.std = nn.Parameter(torch.ones(1, num_outputs))\n",
    "        \n",
    "        #self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)        \n",
    "        dist  = Normal(mu, self.std)\n",
    "        return dist, value\n",
    "\n",
    "class A2C_model(nn.Module):\n",
    "    def __init__(self, params, input_dim , act_size):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.fc1 = nn.Linear(input_dim , self.params['hidden_dim'])\n",
    "        self.actor_fc = nn.Linear(self.params['hidden_dim'],\n",
    "                                  self.params['hidden_dim'])\n",
    "        self.actor_out = nn.Linear(self.params['hidden_dim'], act_size)\n",
    "        self.std = nn.Parameter(torch.ones(1, act_size))\n",
    "        self.critic_fc = nn.Linear(self.params['hidden_dim'],\n",
    "                                   self.params['hidden_dim'])\n",
    "        self.critic_out = nn.Linear(self.params['hidden_dim'], 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        mean = self.actor_out(F.relu(self.actor_fc(x)))\n",
    "        dist = torch.distributions.Normal(mean, self.std)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        value = self.critic_out(F.relu(self.critic_fc(x)))\n",
    "        return torch.clamp(action, -1, 1), log_prob, value\n",
    "    \n",
    "class Agent_A2c():\n",
    "    def __init__(self, device, num_agents, params, state_size, action_size):\n",
    "        self.model = A2C_model(params, state_size, action_size).to(device)\n",
    "        self.device = device\n",
    "        self.num_agents = num_agents\n",
    "        self.params = params\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),\n",
    "                                    lr=self.params['lr'])\n",
    "\n",
    "    def act(self, states):\n",
    "        # mu, std, val, etp = self.model(states)\n",
    "        actions, log_prob, val = self.model(states)\n",
    "        return actions, log_prob, val\n",
    "\n",
    "    def step(self, experiences):\n",
    "        '''\n",
    "            experiences:\n",
    "                    actions (num agents * num actions)\n",
    "                    rewards (size = num agents)\n",
    "                    log_probs (num agents * num actions)\n",
    "                    not_dones (size = num agents)\n",
    "                    state_values (size = num agents)\n",
    "        '''\n",
    "        \n",
    "        actions, rewards, log_probs, not_dones, state_values = experiences\n",
    "        rewards = torch.FloatTensor(rewards).transpose(0, 1).contiguous()                      \n",
    "        processed_experience = [None] * (len(experiences[0]) - 1)\n",
    "        # MDP property\n",
    "        return_  = state_values[-1].detach()\n",
    "        for i in reversed(range(len(experiences[0])-1)):\n",
    "            not_done_ = torch.FloatTensor(not_dones[i+1]).to(device).unsqueeze(1)\n",
    "            reward_ = torch.FloatTensor(rewards[:,i]).to(device).unsqueeze(1)\n",
    "            return_ = reward_ + self.params['gamma'] * not_done_ * return_\n",
    "            next_value_ = state_values[i+1]\n",
    "            advantage_  = reward_ + self.params['gamma'] * not_done_ * next_value_.detach() - state_values[i].detach()\n",
    "            processed_experience[i] = [log_probs[i], advantage_, state_values[i], return_]\n",
    "        log_probs, advantages, values, returns = map(\n",
    "                lambda x: torch.cat(x, dim=0), zip(*processed_experience))\n",
    "        policy_loss = -log_probs * advantages\n",
    "        value_loss = 0.5 * (returns - values).pow(2)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = (policy_loss + value_loss).mean()\n",
    "        # In case the model is not stable\n",
    "        if torch.isnan(loss).any():\n",
    "            print('Nan in loss function')\n",
    "            pass\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(),\n",
    "                                 self.params['grad_clip'])\n",
    "        self.optimizer.step()\n",
    "\n",
    "class A2CAgent():\n",
    "    def __init__(self, device, num_agents, params, state_size, action_size):\n",
    "        self.model = ActorCritic(state_size, action_size, params['hidden_dim']).to(device)\n",
    "        self.device = device\n",
    "        self.num_agents = num_agents\n",
    "        self.params = params\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.params['lr'])\n",
    "        \n",
    "        self.memory = Experience()\n",
    "        \n",
    "        self.t_step = 0\n",
    "\n",
    "    def act(self, states):\n",
    "        # mu, std, val, etp = self.model(states)\n",
    "        actions, val = self.model(states)\n",
    "        return actions, val\n",
    "    \n",
    "    def step(self, actions, rewards, log_probs, not_dones, state_values):\n",
    "        self.memory.add(actions, rewards, log_probs, not_dones, state_values)\n",
    "        \n",
    "        self.t_step = (self.t_step + 1)\n",
    "        if self.t_step % 5 == 0:\n",
    "            self.learn(self.memory.spit(), self.params['gamma'])\n",
    "            self.memory = Experience()\n",
    "    \n",
    "    def compute_returns(self, next_value, rewards, masks, gamma=0.99):\n",
    "        R = next_value\n",
    "        returns = []\n",
    "        for step in reversed(range(len(rewards))):        \n",
    "            R = rewards[:,step] + gamma * R * masks[:,step]\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "    \n",
    "    def compute_gae(self, rewards, masks, values, gamma=0.99, tau=0.95):        \n",
    "        gae = 0\n",
    "        returns = []\n",
    "        \n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "            gae = delta + gamma * tau * masks[step] * gae\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return returns\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        actions, rewards, log_probs, not_dones, state_values = experiences\n",
    "        rewards = torch.FloatTensor(rewards).transpose(0, 1).contiguous()                      \n",
    "        processed_experience = [None] * (len(experiences[0]) - 1)\n",
    "        \n",
    "        # MDP property\n",
    "        return_  = state_values[-1]\n",
    "        for i in reversed(range(len(experiences[0])-1)):\n",
    "            not_done_ = torch.FloatTensor(not_dones[i+1]).to(device).unsqueeze(1)\n",
    "            reward_ = torch.FloatTensor(rewards[:,i]).to(device).unsqueeze(1)\n",
    "            return_ = reward_ + gamma * not_done_ * return_\n",
    "            next_value_ = state_values[i+1]\n",
    "            advantage_  = reward_ + gamma * not_done_ * next_value_.detach() - state_values[i].detach()\n",
    "            processed_experience[i] = [log_probs[i], advantage_, state_values[i], return_]\n",
    "        \n",
    "        log_probs, advantages, values, returns = map(\n",
    "                lambda x: torch.cat(x, dim=0), zip(*processed_experience))\n",
    "        \n",
    "        policy_loss = -log_probs * advantages\n",
    "        value_loss = 0.5 * (returns - values).pow(2)\n",
    "        loss = (policy_loss + value_loss).mean()\n",
    "        \n",
    "        self.optimizer.zero_grad()        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(),\n",
    "                                 self.params['grad_clip'])\n",
    "        self.optimizer.step()\n",
    "\n",
    "    \n",
    "class Experience():\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.not_dones = []\n",
    "        self.state_values = []\n",
    "\n",
    "    def add(self, actions, rewards, log_probs, not_dones, state_values):\n",
    "        self.actions.append(actions)\n",
    "        self.rewards.append(rewards)\n",
    "        self.log_probs.append(log_probs)\n",
    "        self.not_dones.append(not_dones)\n",
    "        self.state_values.append(state_values)\n",
    "\n",
    "    def spit(self):\n",
    "        return (self.actions, self.rewards, self.log_probs, self.not_dones,\n",
    "                self.state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from model import Agent_A2c\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(420)\n",
    "\n",
    "params = {\n",
    "    \"hidden_dim\": 512,\n",
    "    \"gamma\": 0.95,\n",
    "    \"GAE\": 0.99,\n",
    "    \"lr\": 0.0001,\n",
    "    \"grad_clip\": 5,\n",
    "    \"working_dir\": \"./weights.pth\"\n",
    "    }\n",
    "\n",
    "agent = Agent_A2c(device, num_agents, params, state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: 0.2619999941438437, 0.2619999941438437\n",
      "Episode 1: 0.07949999822303652, 0.1707499961834401\n",
      "Episode 2: 0.19799999557435513, 0.1798333293137451\n",
      "Episode 3: 0.9184999794699251, 0.36449999185279014\n",
      "Episode 4: 0.7114999840967358, 0.43389999030157933\n",
      "Episode 5: 0.6059999864548444, 0.4625833229937902\n",
      "Episode 6: 0.7024999842979014, 0.49685713175152035\n",
      "Episode 7: 0.9544999786652625, 0.5540624876157381\n",
      "Episode 8: 0.8454999811016023, 0.5864444313363896\n",
      "Episode 9: 0.8779999803751707, 0.6155999862402677\n",
      "Episode 10: 0.9974999777041376, 0.6503181672824376\n",
      "Episode 11: 0.9794999781064689, 0.6777499848511069\n",
      "Episode 12: 1.1684999738819897, 0.7154999840073286\n",
      "Episode 13: 1.1704999738372863, 0.747999983280897\n",
      "Episode 14: 1.1934999733231961, 0.7776999826170503\n",
      "Episode 15: 1.5549999652430415, 0.8262812315311747\n",
      "Episode 16: 1.7144999616779386, 0.8785293921280433\n",
      "Episode 17: 1.5014999664388597, 0.9131388684786441\n",
      "Episode 18: 1.1534999742172658, 0.9257894529912031\n",
      "Episode 19: 1.3844999690540134, 0.9487249787943435\n",
      "Episode 20: 1.5129999661818148, 0.975595216288985\n",
      "Episode 21: 1.6744999625720083, 1.0073636138473043\n",
      "Episode 22: 1.4724999670870602, 1.0275869335533805\n",
      "Episode 23: 1.870499958191067, 1.062708309579951\n",
      "Episode 24: 1.8699999582022429, 1.0949999755248427\n",
      "Episode 25: 1.9299999568611383, 1.1271153594223926\n",
      "Episode 26: 2.0699999537318945, 1.1620370110634852\n",
      "Episode 27: 1.5399999655783176, 1.1755356880104435\n",
      "Episode 28: 1.7299999613314867, 1.1946551457111692\n",
      "Episode 29: 1.9139999572187663, 1.2186333060947556\n",
      "Episode 30: 1.9959999553859233, 1.2437096496202773\n",
      "Episode 31: 1.7449999609962106, 1.2593749718507752\n",
      "Episode 32: 2.1599999517202377, 1.2866666379074256\n",
      "Episode 33: 2.515999943763018, 1.3228234998443549\n",
      "Episode 34: 2.5179999437183143, 1.3569713982407536\n",
      "Episode 35: 2.636999941058457, 1.3925277466523565\n",
      "Episode 36: 2.969999933615327, 1.4351621300837882\n",
      "Episode 37: 2.6049999417737126, 1.4659473356545756\n",
      "Episode 38: 3.5529999205842615, 1.5194615044989266\n",
      "Episode 39: 1.8754999580793084, 1.5283624658384363\n",
      "Episode 40: 1.7509999608621, 1.5337926486438915\n",
      "Episode 41: 2.432499945629388, 1.5551904414292603\n",
      "Episode 42: 3.213499928172678, 1.5937557783302703\n",
      "Episode 43: 3.690499917510897, 1.641409054220739\n",
      "Episode 44: 3.1659999292343857, 1.6752888514432644\n",
      "Episode 45: 4.150499907229095, 1.7290977874386084\n",
      "Episode 46: 4.202499906066805, 1.7817233644306978\n",
      "Episode 47: 4.898999890498817, 1.8466666253904502\n",
      "Episode 48: 4.830499892029911, 1.9075611818524798\n",
      "Episode 49: 5.839499869477004, 1.9861999556049705\n",
      "Episode 50: 5.035499887447804, 2.0459901503469866\n",
      "Episode 51: 5.954499866906554, 2.121153798742363\n",
      "Episode 52: 6.606999852322042, 2.2057924035268854\n",
      "Episode 53: 7.019499843101949, 2.294935133889387\n",
      "Episode 54: 6.460499855596572, 2.3706726742840627\n",
      "Episode 55: 7.329499836172909, 2.459223159317792\n",
      "Episode 56: 6.04849986480549, 2.522192926080734\n",
      "Episode 57: 6.2744998597539965, 2.5868878732130316\n",
      "Episode 58: 7.4614998332224785, 2.6695084149081074\n",
      "Episode 59: 7.623499829601497, 2.7520749384863303\n",
      "Episode 60: 9.658999784104527, 2.8653032146440056\n",
      "Episode 61: 9.63049978474155, 2.9744192883552563\n",
      "Episode 62: 8.68349980590865, 3.065039614030707\n",
      "Episode 63: 9.516999787278474, 3.1658514917377034\n",
      "Episode 64: 11.225499749090522, 3.289846080312362\n",
      "Episode 65: 10.72749976022169, 3.402537802735231\n",
      "Episode 66: 14.281999680772424, 3.5649178307656366\n",
      "Episode 67: 15.50099965352565, 3.740448445806225\n",
      "Episode 68: 19.245999569818377, 3.965166578038285\n",
      "Episode 69: 21.039999529719353, 4.2090927630623005\n",
      "Episode 70: 23.575999473035335, 4.481866097005582\n",
      "Episode 71: 26.93299939800054, 4.7936873928527355\n",
      "Episode 72: 25.457499430980533, 5.076753311183253\n",
      "Episode 73: 20.74399953633547, 5.2884728547663915\n",
      "Episode 74: 21.631499516498298, 5.506379876922817\n",
      "Episode 75: 13.39149970067665, 5.610131453551157\n",
      "Episode 76: 25.449499431159346, 5.867785583130484\n",
      "Episode 77: 25.45599943101406, 6.118916529898222\n",
      "Episode 78: 21.760999513603746, 6.316917580324874\n",
      "Episode 79: 19.179499571304767, 6.477699855212121\n",
      "Episode 80: 23.469499475415795, 6.687475159165253\n",
      "Episode 81: 25.090999439172446, 6.911908382092171\n",
      "Episode 82: 16.374499634001403, 7.025915505609149\n",
      "Episode 83: 10.894999756477773, 7.071976032405204\n",
      "Episode 84: 15.786999647133053, 7.174505721990237\n",
      "Episode 85: 14.978499665204435, 7.265249837609008\n",
      "Episode 86: 12.500999720580875, 7.325430870746616\n",
      "Episode 87: 11.51349974265322, 7.373022562472827\n",
      "Episode 88: 17.222999615035953, 7.48369646193983\n",
      "Episode 89: 27.12949939360842, 7.701983161180591\n",
      "Episode 90: 24.60549945002422, 7.8877360874316205\n",
      "Episode 91: 26.157499415334314, 8.086320471430563\n",
      "Episode 92: 32.9974992624484, 8.354182608968388\n",
      "Episode 93: 22.55449949586764, 8.505249809892849\n",
      "Episode 94: 34.92349921939895, 8.783336645782388\n",
      "Episode 95: 25.43149943156168, 8.956755008134254\n",
      "Episode 96: 18.252499592024833, 9.052587426524878\n",
      "Episode 97: 28.44949936410412, 9.250515099357319\n",
      "Episode 98: 27.50599938519299, 9.434913930527378\n",
      "Episode 99: 30.019499329011886, 9.640759784512221\n",
      "Episode 100: 28.87449935460463, 9.926884778116829\n",
      "Episode 101: 17.739499603491275, 10.10348477416951\n",
      "Episode 102: 17.950999598763882, 10.281014770201407\n",
      "Episode 103: 23.77249946864322, 10.509554765093139\n",
      "Episode 104: 27.546999384276567, 10.777909759094939\n",
      "Episode 105: 32.4234992752783, 11.096084751983174\n",
      "Episode 106: 26.352499410975724, 11.352584746249953\n",
      "Episode 107: 31.811499288957567, 11.661154739352874\n",
      "Episode 108: 21.341499522980303, 11.866114734771662\n",
      "Episode 109: 20.75149953616783, 12.064849730329586\n",
      "Episode 110: 27.532999384589495, 12.330204724398438\n",
      "Episode 111: 28.246499368641526, 12.60287471830379\n",
      "Episode 112: 28.01099937390536, 12.871299712304024\n",
      "Episode 113: 28.137499371077865, 13.140969706276433\n",
      "Episode 114: 25.64599942676723, 13.385494700810876\n",
      "Episode 115: 25.871499421726913, 13.628659695375712\n",
      "Episode 116: 26.688999403454364, 13.878404689793474\n",
      "Episode 117: 34.2014992355369, 14.205404682484454\n",
      "Episode 118: 34.100999237783256, 14.534879675120115\n",
      "Episode 119: 33.929499241616575, 14.860329667845741\n",
      "Episode 120: 14.370499678794294, 14.988904664971864\n",
      "Episode 121: 13.8939996894449, 15.111099662240594\n",
      "Episode 122: 19.329499567952006, 15.289669658249245\n",
      "Episode 123: 17.17199961617589, 15.442684654829092\n",
      "Episode 124: 21.795999512821435, 15.641944650375285\n",
      "Episode 125: 30.12649932662025, 15.923909644072879\n",
      "Episode 126: 32.95049926349893, 16.23271463717055\n",
      "Episode 127: 33.31299925539643, 16.550444630068725\n",
      "Episode 128: 35.840999198891225, 16.891554622444325\n",
      "Episode 129: 36.538499183300885, 17.237799614705146\n",
      "Episode 130: 34.154499236587434, 17.559384607517163\n",
      "Episode 131: 36.29799918867648, 17.904914599793965\n",
      "Episode 132: 35.554499205294995, 18.23885959232971\n",
      "Episode 133: 37.42399916350841, 18.587939584527163\n",
      "Episode 134: 37.513999161496756, 18.93789957670495\n",
      "Episode 135: 38.2109991459176, 19.29363956875354\n",
      "Episode 136: 38.745999133959415, 19.651399560756982\n",
      "Episode 137: 38.572999137826265, 20.011079552717508\n",
      "Episode 138: 38.44599914066494, 20.360009544918313\n",
      "Episode 139: 32.24799927920103, 20.663734538129532\n",
      "Episode 140: 37.77449915567413, 21.02396953007765\n",
      "Episode 141: 38.85949913142249, 21.38823952193559\n",
      "Episode 142: 36.34949918752536, 21.719599514529115\n",
      "Episode 143: 33.31649925531819, 22.01585950790719\n",
      "Episode 144: 34.22499923501164, 22.326449500964962\n",
      "Episode 145: 34.56549922740087, 22.630599494166674\n",
      "Episode 146: 37.43349916329608, 22.962909486738962\n",
      "Episode 147: 36.85749917617068, 23.282494479595684\n",
      "Episode 148: 37.3904991642572, 23.60809447231795\n",
      "Episode 149: 38.38949914192781, 23.93359446504246\n",
      "Episode 150: 38.71949913455173, 24.270434457513506\n",
      "Episode 151: 38.53499913867563, 24.596239450231195\n",
      "Episode 152: 38.56899913791567, 24.91585944308714\n",
      "Episode 153: 34.62849922599271, 25.191949436916037\n",
      "Episode 154: 38.71349913468585, 25.51447942970693\n",
      "Episode 155: 38.90199913047254, 25.830204422649928\n",
      "Episode 156: 37.934999152086675, 26.14906941552274\n",
      "Episode 157: 36.49799918420613, 26.451304408767257\n",
      "Episode 158: 34.16549923634157, 26.71834440279845\n",
      "Episode 159: 32.53599927276373, 26.96746939723007\n",
      "Episode 160: 29.952499330509454, 27.170404392694117\n",
      "Episode 161: 35.01149921743199, 27.424214387021024\n",
      "Episode 162: 28.103499371837824, 27.61841438268032\n",
      "Episode 163: 28.102499371860176, 27.804269378526136\n",
      "Episode 164: 27.310499389562757, 27.96511937493086\n",
      "Episode 165: 30.238499324116855, 28.160229370569805\n",
      "Episode 166: 28.43349936446175, 28.301744367406698\n",
      "Episode 167: 28.08599937222898, 28.42759436459373\n",
      "Episode 168: 30.327999322116376, 28.53841436211671\n",
      "Episode 169: 33.46749925194308, 28.66268935933895\n",
      "Episode 170: 34.312999233044685, 28.770059356939047\n",
      "Episode 171: 36.380999186821285, 28.864539354827247\n",
      "Episode 172: 36.25899918954819, 28.97255435241293\n",
      "Episode 173: 34.76599922291935, 29.112774349278766\n",
      "Episode 174: 23.03999948501587, 29.126859348963944\n",
      "Episode 175: 19.906999555043875, 29.19201434750762\n",
      "Episode 176: 25.791999423503874, 29.195439347431066\n",
      "Episode 177: 26.568999406136573, 29.206569347182295\n",
      "Episode 178: 22.378499499801546, 29.212744347044268\n",
      "Episode 179: 28.414999364875257, 29.305099344979975\n",
      "Episode 180: 25.026499440614135, 29.320669344631956\n",
      "Episode 181: 15.110499662254005, 29.22086434686277\n",
      "Episode 182: 23.56349947331473, 29.292754345255904\n",
      "Episode 183: 22.13199950531125, 29.405124342744237\n",
      "Episode 184: 22.15499950479716, 29.46880434132088\n",
      "Episode 185: 28.859999354928732, 29.607619338218125\n",
      "Episode 186: 32.96599926315248, 29.81226933364384\n",
      "Episode 187: 34.03499923925847, 30.03748432860989\n",
      "Envinroment solved in episode187!\n",
      "Score: deque([17.222999615035953, 27.12949939360842, 24.60549945002422, 26.157499415334314, 32.9974992624484, 22.55449949586764, 34.92349921939895, 25.43149943156168, 18.252499592024833, 28.44949936410412, 27.50599938519299, 30.019499329011886, 28.87449935460463, 17.739499603491275, 17.950999598763882, 23.77249946864322, 27.546999384276567, 32.4234992752783, 26.352499410975724, 31.811499288957567, 21.341499522980303, 20.75149953616783, 27.532999384589495, 28.246499368641526, 28.01099937390536, 28.137499371077865, 25.64599942676723, 25.871499421726913, 26.688999403454364, 34.2014992355369, 34.100999237783256, 33.929499241616575, 14.370499678794294, 13.8939996894449, 19.329499567952006, 17.17199961617589, 21.795999512821435, 30.12649932662025, 32.95049926349893, 33.31299925539643, 35.840999198891225, 36.538499183300885, 34.154499236587434, 36.29799918867648, 35.554499205294995, 37.42399916350841, 37.513999161496756, 38.2109991459176, 38.745999133959415, 38.572999137826265, 38.44599914066494, 32.24799927920103, 37.77449915567413, 38.85949913142249, 36.34949918752536, 33.31649925531819, 34.22499923501164, 34.56549922740087, 37.43349916329608, 36.85749917617068, 37.3904991642572, 38.38949914192781, 38.71949913455173, 38.53499913867563, 38.56899913791567, 34.62849922599271, 38.71349913468585, 38.90199913047254, 37.934999152086675, 36.49799918420613, 34.16549923634157, 32.53599927276373, 29.952499330509454, 35.01149921743199, 28.103499371837824, 28.102499371860176, 27.310499389562757, 30.238499324116855, 28.43349936446175, 28.08599937222898, 30.327999322116376, 33.46749925194308, 34.312999233044685, 36.380999186821285, 36.25899918954819, 34.76599922291935, 23.03999948501587, 19.906999555043875, 25.791999423503874, 26.568999406136573, 22.378499499801546, 28.414999364875257, 25.026499440614135, 15.110499662254005, 23.56349947331473, 22.13199950531125, 22.15499950479716, 28.859999354928732, 32.96599926315248, 34.03499923925847], maxlen=100)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "def plot_scores(scores):\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "    \n",
    "scores_window = deque(maxlen=100)\n",
    "for i in range(500):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states_ = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    memories = Experience()\n",
    "    done = [False] * num_agents\n",
    "    steps = 0\n",
    "    while True:\n",
    "        #dist, state_values_ = agent.act(torch.FloatTensor(states).to(device))\n",
    "        #actions_ = torch.clamp(dist.sample(), -1, 1) #dist.sample()\n",
    "        actions_, log_prob_, state_values_ = agent.act(\n",
    "                torch.FloatTensor(states).to(device))\n",
    "        env_info = env.step(actions_.detach().cpu().numpy())[brain_name]\n",
    "        next_states_ = env_info.vector_observations\n",
    "        rewards_ = env_info.rewards\n",
    "        done = env_info.local_done\n",
    "        not_done_ = (1 - np.array(done))\n",
    "        \n",
    "        memories.add(actions_, rewards_, log_prob_, not_done_, state_values_) \n",
    "        #agent.step(actions_, rewards_, log_prob_, not_done_, state_values_)\n",
    "        steps += 1\n",
    "        if steps % 5 == 0:\n",
    "            experiences = memories.spit()\n",
    "            agent.step(experiences)\n",
    "            memories = Experience()\n",
    "        states = next_states_\n",
    "        scores += rewards_\n",
    "        if np.any(done):\n",
    "            break\n",
    "        # print(scores)\n",
    "    \n",
    "    scores_window.append(np.mean(scores))\n",
    "    print(f\"Episode {i}: {np.mean(scores)}, {np.mean(scores_window)}\")\n",
    "    \n",
    "    if (len(scores_window)) == 100 and (np.mean(scores_window) > 30):\n",
    "        torch.save(agent.model.state_dict(), params['working_dir'])\n",
    "        print(f\"Envinroment solved in episode{i}!\")\n",
    "        print(f\"Score: {scores_window}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
