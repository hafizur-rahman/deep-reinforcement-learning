{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(            \n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(        \n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "        self.std = nn.Parameter(torch.ones(1, num_outputs))        \n",
    "        \n",
    "    def forward(self, state):\n",
    "        x     = self.fc1(state)\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)        \n",
    "        \n",
    "        dist  = Normal(mu, self.std)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return torch.clamp(action, -1, 1), log_prob, value\n",
    "\n",
    "\n",
    "class A2CAgent():\n",
    "    def __init__(self, device, num_agents, params, state_size, action_size):\n",
    "        self.model = ActorCritic(state_size, action_size, params['hidden_dim']).to(device)\n",
    "        self.device = device\n",
    "        self.num_agents = num_agents\n",
    "        self.params = params\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.params['lr'])\n",
    "\n",
    "    def act(self, states):\n",
    "        # mu, std, val, etp = self.model(states)\n",
    "        actions, log_prob, val = self.model(states)\n",
    "        return actions, log_prob, val\n",
    "\n",
    "    def compute_returns(self, rewards, masks, values, gamma):\n",
    "        R = values[-1].detach()\n",
    "\n",
    "        step_count = (len(rewards[0]) - 1)        \n",
    "        result = [None] * step_count\n",
    "                \n",
    "        for i in reversed(range(step_count)):\n",
    "            mask = torch.FloatTensor(masks[i+1]).to(device).unsqueeze(1)            \n",
    "            reward = torch.FloatTensor(rewards[:,i]).to(device).unsqueeze(1)\n",
    "            \n",
    "            R = reward + gamma * mask * R\n",
    "            next_value = values[i+1]\n",
    "            advantage  = reward + gamma * mask * next_value.detach() - values[i].detach()\n",
    "            \n",
    "            result[i] = [advantage, R]            \n",
    "        return result\n",
    "    \n",
    "    def step(self, experiences):\n",
    "        '''\n",
    "            experiences:\n",
    "                    actions (num agents * num actions)\n",
    "                    rewards (size = num agents)\n",
    "                    log_probs (num agents * num actions)\n",
    "                    not_dones (size = num agents)\n",
    "                    state_values (size = num agents)\n",
    "        '''\n",
    "        actions, rewards, log_probs, not_dones, state_values = experiences\n",
    "        \n",
    "        rewards = torch.FloatTensor(rewards).transpose(0, 1).contiguous()\n",
    "        \n",
    "        result = self.compute_returns(rewards, not_dones, state_values, self.params['gamma'])\n",
    "        advantages, returns = map(lambda x: torch.cat(x, dim=0), zip(*result))\n",
    "\n",
    "        log_probs = torch.cat(log_probs[:-1], dim=0)\n",
    "        values = torch.cat(state_values[:-1], dim=0)\n",
    "        \n",
    "        policy_loss = -log_probs * advantages\n",
    "        value_loss = 0.5 * (returns - values).pow(2)        \n",
    "        loss = (policy_loss + value_loss).mean()\n",
    "        \n",
    "        self.optimizer.zero_grad()        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(),\n",
    "                                 self.params['grad_clip'])\n",
    "        self.optimizer.step()\n",
    "\n",
    "    \n",
    "class Experience():\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.not_dones = []\n",
    "        self.state_values = []\n",
    "\n",
    "    def add(self, actions, rewards, log_probs, not_dones, state_values):\n",
    "        self.actions.append(actions)\n",
    "        self.rewards.append(rewards)\n",
    "        self.log_probs.append(log_probs)\n",
    "        self.not_dones.append(not_dones)\n",
    "        self.state_values.append(state_values)\n",
    "\n",
    "    def spit(self):\n",
    "        return (self.actions, self.rewards, self.log_probs, self.not_dones,\n",
    "                self.state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(499)\n",
    "\n",
    "params = {\n",
    "    \"hidden_dim\": 512,\n",
    "    \"gamma\": 0.95,\n",
    "    \"GAE\": 0.99,\n",
    "    \"lr\": 0.0001,\n",
    "    \"grad_clip\": 5,\n",
    "    \"working_dir\": \"./weights.pth\"\n",
    "    }\n",
    "\n",
    "agent = A2CAgent(device, num_agents, params, state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: 0.5434999878518283, 0.5434999878518283\n",
      "Episode 1: 0.348499992210418, 0.4459999900311231\n",
      "Episode 2: 0.6579999852925539, 0.5166666551182667\n",
      "Episode 3: 0.7214999838732183, 0.5678749873070046\n",
      "Episode 4: 0.7694999828003347, 0.6081999864056706\n",
      "Episode 5: 0.8469999810680747, 0.6479999855160713\n",
      "Episode 6: 0.9794999781064689, 0.6953571273146996\n",
      "Episode 7: 1.2119999729096889, 0.7599374830140732\n",
      "Episode 8: 0.6904999845661223, 0.7522222054087453\n",
      "Episode 9: 0.8529999809339642, 0.7622999829612671\n",
      "Episode 10: 1.0499999765306711, 0.7884545278312128\n",
      "Episode 11: 0.4119999907910824, 0.757083316411202\n",
      "Episode 12: 0.4599999897181988, 0.7342307528194325\n",
      "Episode 13: 1.111499975156039, 0.7611785544149045\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "def plot_scores(scores):\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "    \n",
    "scores_window = deque(maxlen=100)\n",
    "for i in range(500):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states_ = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    memories = Experience()\n",
    "\n",
    "    steps = 0\n",
    "    while True:\n",
    "        actions_, log_prob_, state_values_ = agent.act(torch.FloatTensor(states).to(device))\n",
    "        env_info = env.step(actions_.detach().cpu().numpy())[brain_name]\n",
    "        next_states_ = env_info.vector_observations\n",
    "        rewards_ = env_info.rewards\n",
    "        done = env_info.local_done\n",
    "        not_done_ = (1 - np.array(done))\n",
    "        \n",
    "        memories.add(actions_, rewards_, log_prob_, not_done_, state_values_) \n",
    "\n",
    "        steps += 1\n",
    "        if steps % 5 == 0:\n",
    "            experiences = memories.spit()\n",
    "            agent.step(experiences)\n",
    "            memories = Experience()\n",
    "        states = next_states_\n",
    "        scores += rewards_\n",
    "        if np.any(done):\n",
    "            break\n",
    "        # print(scores)\n",
    "    \n",
    "    scores_window.append(np.mean(scores))\n",
    "    print(f\"Episode {i}: {np.mean(scores)}, {np.mean(scores_window)}\")\n",
    "    \n",
    "    if (len(scores_window)) == 100 and (np.mean(scores_window) > 30):\n",
    "        torch.save(agent.model.state_dict(), params['working_dir'])\n",
    "        print(f\"Envinroment solved in episode{i}!\")\n",
    "        print(f\"Score: {scores_window}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
